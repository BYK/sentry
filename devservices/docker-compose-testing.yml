x-restart-policy: &restart_policy
  restart: unless-stopped
x-depends_on-healthy: &depends_on-healthy
  condition: service_healthy
x-depends_on-default: &depends_on-default
  condition: service_started
x-healthcheck-defaults: &healthcheck_defaults
  interval: 30s
  timeout: 1m30s
  retries: 10
  start_period: 10s
services:
  memcached:
    <<: *restart_policy
    image: 'ghcr.io/getsentry/image-mirror-library-memcached:1.5-alpine'
    command: ['-I', '${SENTRY_MAX_EXTERNAL_SOURCEMAP_SIZE:-1M}']
    healthcheck:
      <<: *healthcheck_defaults
      test: echo stats | nc 127.0.0.1 11211
  redis:
    <<: *restart_policy
    image: ghcr.io/getsentry/image-mirror-library-redis:5.0-alpine
    healthcheck:
      <<: *healthcheck_defaults
      test: redis-cli ping
    volumes:
      - 'sentry-redis:/data'
    ulimits:
      nofile:
        soft: 10032
        hard: 10032
    ports:
      - '6379:6379'
  postgres:
    <<: *restart_policy
    # Using the same postgres version as Sentry dev for consistency purposes
    image: 'ghcr.io/getsentry/image-mirror-library-postgres:14-alpine'
    healthcheck:
      <<: *healthcheck_defaults
      # Using default user "postgres" from sentry/sentry.conf.example.py or value of POSTGRES_USER if provided
      test: ['CMD-SHELL', 'pg_isready -U ${POSTGRES_USER:-postgres}']
    command: ['postgres', '-c', 'max_connections=${POSTGRES_MAX_CONNECTIONS:-100}']
    environment:
      POSTGRES_HOST_AUTH_METHOD: 'trust'
      POSTGRES_DB: 'sentry'
    volumes:
      - 'sentry-postgres:/var/lib/postgresql/data'
  kafka:
    <<: *restart_policy
    image: 'ghcr.io/getsentry/image-mirror-confluentinc-cp-kafka:7.5.0'
    environment:
      # https://docs.confluent.io/platform/current/installation/docker/config-reference.html#cp-kakfa-example
      KAFKA_PROCESS_ROLES: 'broker,controller'
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1001@127.0.0.1:29093'
      KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'
      KAFKA_NODE_ID: '1001'
      CLUSTER_ID: 'MkU3OEVBNTcwNTJENDM2Qk'
      KAFKA_LISTENERS: 'PLAINTEXT://0.0.0.0:29092,INTERNAL://0.0.0.0:9093,EXTERNAL://0.0.0.0:9092,CONTROLLER://0.0.0.0:29093'
      KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://127.0.0.1:29092,INTERNAL://kafka:9093,EXTERNAL://kafka:9092'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'PLAINTEXT:PLAINTEXT,INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT'
      KAFKA_INTER_BROKER_LISTENER_NAME: 'PLAINTEXT'
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: '1'
      KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS: '1'
      KAFKA_LOG_RETENTION_HOURS: '24'
      KAFKA_MESSAGE_MAX_BYTES: '50000000' #50MB or bust
      KAFKA_MAX_REQUEST_SIZE: '50000000' #50MB on requests apparently too
      CONFLUENT_SUPPORT_METRICS_ENABLE: 'false'
      KAFKA_LOG4J_LOGGERS: 'kafka.cluster=WARN,kafka.controller=WARN,kafka.coordinator=WARN,kafka.log=WARN,kafka.server=WARN,state.change.logger=WARN'
      KAFKA_LOG4J_ROOT_LOGLEVEL: 'WARN'
      KAFKA_TOOLS_LOG4J_LOGLEVEL: 'WARN'
    ulimits:
      nofile:
        soft: 4096
        hard: 4096
    volumes:
      - 'sentry-kafka:/var/lib/kafka/data'
      - 'sentry-kafka-log:/var/lib/kafka/log'
    healthcheck:
      <<: *healthcheck_defaults
      test: ['CMD-SHELL', 'nc -z localhost 9092']
      interval: 10s
      timeout: 10s
      retries: 30
  clickhouse:
    <<: *restart_policy
    image: 'ghcr.io/getsentry/image-mirror-altinity-clickhouse-server:23.3.19.33.altinitystable'
    ulimits:
      nofile:
        soft: 262144
        hard: 262144
    volumes:
      - 'sentry-clickhouse:/var/lib/clickhouse'
      - 'sentry-clickhouse-log:/var/log/clickhouse-server'
      - type: bind
        read_only: true
        source: ./clickhouse/config.xml
        target: /etc/clickhouse-server/config.d/sentry.xml
    environment:
      # This limits Clickhouse's memory to 30% of the host memory
      # If you have high volume and your search return incomplete results
      # You might want to change this to a higher value (and ensure your host has enough memory)
      MAX_MEMORY_USAGE_RATIO: 0.3
    healthcheck:
      test: [
          'CMD-SHELL',
          # Manually override any http_proxy envvar that might be set, because
          # this wget does not support no_proxy. See:
          # https://github.com/getsentry/self-hosted/issues/1537
          "http_proxy='' wget -nv -t1 --spider 'http://localhost:8123/' || exit 1",
        ]
      interval: 10s
      timeout: 10s
      retries: 30
    ports:
      - '8123:8123'
      - '9000:9000'
      - '9009:9009'
  symbolicator:
    <<: *restart_policy
    image: 'us-central1-docker.pkg.dev/sentryio/symbolicator/image:nightly'
    volumes:
      - 'sentry-symbolicator:/data'
      - type: bind
        read_only: true
        source: ./symbolicator
        target: /etc/symbolicator
    command: run -c /etc/symbolicator/config.yml
    ports:
      - '3021:3021'
  relay:
    <<: *restart_policy
    image: 'us-central1-docker.pkg.dev/sentryio/relay/relay:nightly'
    volumes:
      - type: bind
        read_only: true
        source: ./relay
        target: /work/.relay
      - type: bind
        read_only: true
        source: ./geoip
        target: /geoip
    depends_on:
      kafka:
        <<: *depends_on-healthy
      redis:
        <<: *depends_on-healthy
  vroom:
    <<: *restart_policy
    image: 'us-central1-docker.pkg.dev/sentryio/vroom/vroom:latest'
    environment:
      SENTRY_KAFKA_BROKERS_PROFILING: 'kafka:9092'
      SENTRY_KAFKA_BROKERS_OCCURRENCES: 'kafka:9092'
      SENTRY_BUCKET_PROFILES: file://localhost//var/lib/sentry-profiles
      SENTRY_SNUBA_HOST: 'http://snuba-api:1218'
    volumes:
      - sentry-vroom:/var/lib/sentry-profiles
    depends_on:
      kafka:
        <<: *depends_on-healthy
    profiles:
      - feature-complete
  snuba:
    image: ghcr.io/getsentry/snuba:latest
    ports:
      - '1218:1218'
      - '1219:1219'
    command: ['devserver']
    environment:
      PYTHONUNBUFFERED: '1'
      SNUBA_SETTINGS: docker
      DEBUG: '1'
      CLICKHOUSE_HOST: 'clickhouse'
      CLICKHOUSE_PORT: '9000'
      CLICKHOUSE_HTTP_PORT: '8123'
      DEFAULT_BROKERS: 'kafka:9093'
      REDIS_HOST: '${REDIS_HOST:-redis}'
      REDIS_PORT: '6379'
      REDIS_DB: '1'
      ENABLE_SENTRY_METRICS_DEV: '${ENABLE_SENTRY_METRICS_DEV:-}'
      ENABLE_PROFILES_CONSUMER: '${ENABLE_PROFILES_CONSUMER:-}'
      ENABLE_SPANS_CONSUMER: '${ENABLE_SPANS_CONSUMER:-}'
      ENABLE_ISSUE_OCCURRENCE_CONSUMER: '${ENABLE_ISSUE_OCCURRENCE_CONSUMER:-}'
      ENABLE_AUTORUN_MIGRATION_SEARCH_ISSUES: '1'
      ENABLE_GROUP_ATTRIBUTES_CONSUMER: '${ENABLE_GROUP_ATTRIBUTES_CONSUMER:-}'
    platform: linux/amd64
    depends_on:
      - kafka
      - redis
      - clickhouse
  bigtable:
    image: 'us.gcr.io/sentryio/cbtemulator:23c02d92c7a1747068eb1fc57dddbad23907d614'
    ports:
      - '8086:8086'
  redis-cluster:
    image: ghcr.io/getsentry/docker-redis-cluster:7.0.10
    ports:
      - '7000:7000'
      - '7001:7001'
      - '7002:7002'
      - '7003:7003'
      - '7004:7004'
      - '7005:7005'
    volumes:
      - sentry-redis-cluster:/redis-data
    environment:
      - IP=0.0.0.0

volumes:
  # These store application data that should persist across restarts.
  sentry-data:
    external: true
  sentry-postgres:
    external: true
  sentry-redis:
    external: true
  sentry-redis-cluster:
    external: true
  sentry-kafka:
    external: true
  sentry-clickhouse:
    external: true
  sentry-symbolicator:
    external: true
  # This volume stores profiles and should be persisted.
  # Not being external will still persist data across restarts.
  # It won't persist if someone does a docker compose down -v.
  sentry-vroom:
  sentry-kafka-log:
  sentry-clickhouse-log:
